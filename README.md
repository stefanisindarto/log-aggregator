# Log Aggregator Project

## Overview
The Log Aggregator is a Python-based tool designed to process network flow logs and generate statistics based on the data found in these logs. The tool aggregates information by matching network protocols and ports to specific tags and categorizes each log entry accordingly.

## Project Structure
This section details the files included in the Log Aggregator project and their respective purposes:

### Scripts
- **create_10mb_log.py**: A script designed to generate a 10MB flow log file for testing the application's ability to handle large data files.
- **create_10000_lookup_mapping.py**: This script generates a lookup table with 10,000 mappings to test the application's performance with extensive datasets.

### Data Files
- **flow-log.log**: The flow log file used by the application to extract and aggregate data based on network traffic information.
- **lookup_tables.csv**: Contains the mapping of ports and protocols to specific tags used by the application for categorizing network data.
- **protocol-numbers.csv**: A CSV file that lists protocol numbers and their corresponding names, sourced from IANA for accurate protocol identification.

### Core Application
- **main.py**: The main executable script for the Log Aggregator, which coordinates the reading, processing, and summarizing of network log data.
- **output_results.txt**: This file stores the output generated by `main.py`, detailing the aggregated results from the processed log data.

### Testing
- **test.py**: Contains the unittests for the Log Aggregator project, ensuring all components function as expected under predefined conditions.


## How To Run The Script
- **Navigate to the Scriptâ€™s Directory:** Open your command line interface and navigate to the directory containing the main.py script.
- **Run the Script with Default Parameters:** 
```bash
python3 main.py
```
- **Customizing Parameters:** To specify custom paths for input and output files, use the command-line arguments as follows (Replace path/to/... with the actual paths to your files):
```bash
python3 main.py --lookup_file path/to/lookup.csv --protocol_file path/to/protocol.csv --flow_log_file path/to/log.log --output_file path/to/output.txt
```

## Testing
The Log Aggregator project has been thoroughly tested to ensure its robustness and capability to handle large, complex datasets efficiently. Below are the key aspects of the testing:

### Supported Formats and Versions
- **Log Format**: The application supports only the default log format, specifically version 2.

### Performance Testing
- **Flow Log File Size**: The program successfully processes flow logs greater than 10MB, meeting the requirements for handling substantial data loads.
- **Lookup Table Capacity**: The application efficiently manages lookup tables with up to 10,000 mappings, ensuring no performance degradation even with extensive datasets.

### Data Handling
- **Data Format**: All input files, including logs and tag mappings, are treated as plain text (ASCII) files. This ensures compatibility and ease of handling across different systems.
- **Mapping Flexibility**: The application allows tags to map to multiple port-protocol combinations, enhancing the flexibility of data categorization. This has been validated with test cases that include multiple mappings like `sv_P1` and `sv_P2`.

### Case Sensitivity
- **Case Insensitivity**: The program handles all matches in a case-insensitive manner, ensuring consistent data processing and aggregation regardless of case variations in the input data.

### Test Setup
Tests are conducted using the Python `unittest` framework. The test setup involves:
- Creating mock CSV files (`test_lookup.csv` and `test_protocol.csv`) and a mock flow log file (`test_flow.log`).
- These mock files simulate realistic input data and scenarios that the Log Aggregator might handle.

### Test Execution
To run the tests, navigate to the project directory in your terminal and execute:
```bash
python3 -m unittest
```

## Assumptions
This project makes several assumptions about the data sources and operating environment:

- **Files Location**: All CSV and log files must exist at specified paths prior to running the scripts.
- **CSV File Format**: 
  - **sample-lookup-table.csv**:
    - Must contain headers named `dstport`, `protocol`, and `tag`.
    - `dstport` should always be an integer.
    - `protocol` should be a string that has been lowercased.
    - Each row must contain valid data with no missing fields.
  - **protocol-numbers.csv**:
    - Sourced from [IANA Protocol Numbers](https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml).
- **Unmatched Entries**:
  - If a protocol number is not found in `protocol-numbers.csv`, it defaults to 'others'.
  - If a port-protocol combination is not found in the `sample-lookup-table.csv`, it defaults to 'Untagged'.
- **Case Sensitivity**:
  - To ensure case insensitivity in tag comparisons and storage, all tags and protocols are stored in lowercase. This uniformity helps prevent  issues that can arise from case mismatches in processing log data.
