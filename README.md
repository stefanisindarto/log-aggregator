# Log Aggregator Project

## Overview
The Log Aggregator is a Python-based tool designed to process network flow logs and generate statistics based on the data found in these logs. The tool aggregates information by matching network protocols and ports to specific tags and categorizes each log entry accordingly.

## Project Structure
This section details the files included in the Log Aggregator project and their respective purposes:

- **create_10mb_log.py**: A script designed to generate a 10MB flow log file for testing the application's ability to handle large data files.
- **create_10000_lookup_mapping.py**: This script generates a lookup table with 10,000 mappings to test the application's performance with extensive datasets.
- **flow-log.lg**: The flow log file used by the application to extract and aggregate data based on network traffic information.
- **lookup_tables.csv**: Contains the mapping of ports and protocols to specific tags used by the application for categorizing network data.
- **main.py**: The main executable script for the Log Aggregator, which coordinates the reading, processing, and summarizing of network log data.
- **output_results.txt**: This file stores the output generated by `main.py`, detailing the aggregated results from the processed log data.
- **protocol-numbers.csv**: A CSV file that lists protocol numbers and their corresponding names, sourced from IANA for accurate protocol identification.
- **test.py**: Contains the unittests for the Log Aggregator project, ensuring all components function as expected under predefined conditions.


## How To Run The Script
- **Navigate to the Script’s Directory:** Open your command line interface and navigate to the directory containing the main.py script.
- **Run the Script with Default Parameters:** 
```bash
python3 main.py
```
- **Customizing Parameters:** To specify custom paths for input and output files, use the command-line arguments as follows (Replace path/to/... with the actual paths to your files):
```bash
python3 main.py --lookup_file path/to/lookup.csv --protocol_file path/to/protocol.csv --flow_log_file path/to/log.log --output_file path/to/output.txt
```

## Testing
The project includes comprehensive tests which confirm the application’s robustness and its capability to handle large and complex datasets efficiently:

- **Flow Log File Size:** Successfully handles flow logs greater than 10MB in size, ensuring robust performance even under substantial data loads. According to the requirements, the flow log file size can be up to 10 MB.
- **Lookup Table Capacity:** Proven capability to handle up to 10,000 mappings within the lookup table, catering to extensive datasets without performance degradation. The application can efficiently manage lookup tables with up to 10,000 mappings.
- **Data Format:** Both the input file and the file containing tag mappings are treated as plain text (ASCII) files, ensuring compatibility and ease of handling.
- **Mapping Flexibility:** The application supports tags that can map to multiple port-protocol combinations, enhancing the flexibility of the data categorization, such as mapping `sv_P1` and `sv_P2` as demonstrated in the test cases.
- **Case Insensitivity:** All matches are case insensitive, promoting uniform data processing and aggregation regardless of the case variations in the input data.

### Test Setup
Tests are conducted using the Python `unittest` framework. The test setup involves:
- Creating mock CSV files (`test_lookup.csv` and `test_protocol.csv`) and a mock flow log file (`test_flow.log`).
- These mock files simulate realistic input data and scenarios that the Log Aggregator might handle.

### Test Execution
To run the tests, navigate to the project directory in your terminal and execute:
```bash
python3 -m unittest
```

## Assumptions
This project makes several assumptions about the data sources and operating environment:

- **Files Location**: All CSV and log files must exist at specified paths prior to running the scripts.
- **CSV File Format**: 
  - **sample-lookup-table.csv**:
    - Must contain headers named `dstport`, `protocol`, and `tag`.
    - `dstport` should always be an integer.
    - `protocol` should be a string that has been lowercased.
    - Each row must contain valid data with no missing fields.
  - **protocol-numbers.csv**:
    - Sourced from [IANA Protocol Numbers](https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml).
- **Unmatched Entries**:
  - If a protocol number is not found in `protocol-numbers.csv`, it defaults to 'others'.
  - If a port-protocol combination is not found in the `sample-lookup-table.csv`, it defaults to 'Untagged'.
- **Case Sensitivity**:
  - To ensure case insensitivity in tag comparisons and storage, all tags and protocols are stored in lowercase. This uniformity helps prevent  issues that can arise from case mismatches in processing log data.
